# -*- coding: utf-8 -*-
"""Assignment1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kidJ_SJW_Wsj3dZCS6KmGTYerKgZ-q4R
"""

# STEP1: DATA PROCESSING
import pandas as pd

# Load train and test data
train_df = pd.read_csv("train.csv")
test_df = pd.read_csv("test.csv")

# Check the structure of the data
print(train_df.head())
print(test_df.head())



!pip install matplotlib-venn

# https://pypi.python.org/pypi/pydot
!apt-get -qq install -y graphviz && pip install pydot
import pydot

!pip install cartopy
import cartopy

# STEP2: TEXT EXTRACTION AND IMAGE EXTRACTION
!pip install python-docx

from docx import Document

def extract_text_from_docx(file_path):
    doc = Document('/content/156155545-Rental-Agreement-Kns-Home.pdf.docx')
    text_content = []
    for paragraph in doc.paragraphs:
        text_content.append(paragraph.text)
    return ' '.join(text_content)

# Example usage:
text_file_path = 'path/to/text_document.docx'
text_content = extract_text_from_docx(text_file_path)
print(text_content)

!pip install python-docx

from docx import Document

def extract_text_from_docx(file_path):
    doc = Document('/content/228094620-Rental-Agreement.pdf.docx')
    text_content = []
    for paragraph in doc.paragraphs:
        text_content.append(paragraph.text)
    return ' '.join(text_content)

# Example usage:
text_file_path = 'path/to/text_document.docx'
text_content = extract_text_from_docx(text_file_path)
print(text_content)

import cv2

def extract_features_from_image(file_path):
    image = cv2.imread('/content/24158401-Rental-Agreement.png')
    # Perform image preprocessing and feature extraction
    # Example: Convert image to grayscale and resize
    grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    resized_image = cv2.resize(grayscale_image, (224, 224))  # Example size

    return resized_image

# Example usage:
image_file_path = 'path/to/image_document.png'
image_features = extract_features_from_image(image_file_path)
print(image_features.shape)  # Print shape of the extracted image features

import cv2

def extract_features_from_image(file_path):
    image = cv2.imread('/content/95980236-Rental-Agreement.png')
    # Perform image preprocessing and feature extraction
    # Example: Convert image to grayscale and resize
    grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    resized_image = cv2.resize(grayscale_image, (224, 224))  # Example size

    return resized_image

# Example usage:
image_file_path = 'path/to/image_document.png'
image_features = extract_features_from_image(image_file_path)
print(image_features.shape)  # Print shape of the extracted image features

!pip install python-docx

from docx import Document

def extract_text_from_docx(file_path):
    doc = Document('/content/6683127-House-Rental-Contract-GERALDINE-GALINATO-v2-Page-1.docx')
    text_content = []
    for paragraph in doc.paragraphs:
        text_content.append(paragraph.text)
    return ' '.join(text_content)

# Example usage:
text_file_path = 'path/to/text_document.docx'
text_content = extract_text_from_docx(text_file_path)
print(text_content)

!pip install python-docx

from docx import Document

def extract_text_from_docx(file_path):
    doc = Document('/content/6683129-House-Rental-Contract-Geraldine-Galinato-v2.docx')
    text_content = []
    for paragraph in doc.paragraphs:
        text_content.append(paragraph.text)
    return ' '.join(text_content)

# Example usage:
text_file_path = 'path/to/text_document.docx'
text_content = extract_text_from_docx(text_file_path)
print(text_content)

!pip install python-docx

from docx import Document

def extract_text_from_docx(file_path):
    doc = Document('/content/18325926-Rental-Agreement-1.docx')
    text_content = []
    for paragraph in doc.paragraphs:
        text_content.append(paragraph.text)
    return ' '.join(text_content)

# Example usage:
text_file_path = 'path/to/text_document.docx'
text_content = extract_text_from_docx(text_file_path)
print(text_content)

!pip install python-docx

from docx import Document

def extract_text_from_docx(file_path):
    doc = Document('/content/44737744-Maddireddy-Bhargava-Reddy-Rental-Agreement.docx')
    text_content = []
    for paragraph in doc.paragraphs:
        text_content.append(paragraph.text)
    return ' '.join(text_content)

# Example usage:
text_file_path = 'path/to/text_document.docx'
text_content = extract_text_from_docx(text_file_path)
print(text_content)

!pip install python-docx

from docx import Document

def extract_text_from_docx(file_path):
    doc = Document('/content/46239065-Standard-Rental-Agreement-Rental-With-Performance-Fee.docx')
    text_content = []
    for paragraph in doc.paragraphs:
        text_content.append(paragraph.text)
    return ' '.join(text_content)

# Example usage:
text_file_path = 'path/to/text_document.docx'
text_content = extract_text_from_docx(text_file_path)
print(text_content)

!pip install python-docx

from docx import Document

def extract_text_from_docx(file_path):
    doc = Document('/content/47854715-RENTAL-AGREEMENT.docx')
    text_content = []
    for paragraph in doc.paragraphs:
        text_content.append(paragraph.text)
    return ' '.join(text_content)

# Example usage:
text_file_path = 'path/to/text_document.docx'
text_content = extract_text_from_docx(text_file_path)
print(text_content)

!pip install python-docx

from docx import Document

def extract_text_from_docx(file_path):
    doc = Document('/content/50070534-RENTAL-AGREEMENT.docx')
    text_content = []
    for paragraph in doc.paragraphs:
        text_content.append(paragraph.text)
    return ' '.join(text_content)

# Example usage:
text_file_path = 'path/to/text_document.docx'
text_content = extract_text_from_docx(text_file_path)
print(text_content)

import cv2

def extract_features_from_image(file_path):
    image = cv2.imread('/content/36199312-Rental-Agreement.png')
    # Perform image preprocessing and feature extraction
    # Example: Convert image to grayscale and resize
    grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    resized_image = cv2.resize(grayscale_image, (224, 224))  # Example size

    return resized_image

# Example usage:
image_file_path = 'path/to/image_document.png'
image_features = extract_features_from_image(image_file_path)
print(image_features.shape)  # Print shape of the extracted image features

import cv2

def extract_features_from_image(file_path):
    image = cv2.imread('/content/54770958-Rental-Agreement.png')
    # Perform image preprocessing and feature extraction
    # Example: Convert image to grayscale and resize
    grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    resized_image = cv2.resize(grayscale_image, (224, 224))  # Example size

    return resized_image

# Example usage:
image_file_path = 'path/to/image_document.png'
image_features = extract_features_from_image(image_file_path)
print(image_features.shape)  # Print shape of the extracted image features

def extract_features_from_image(file_path):
    image = cv2.imread('/content/54945838-Rental-Agreement.png')
    # Perform image preprocessing and feature extraction
    # Example: Convert image to grayscale and resize
    grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    resized_image = cv2.resize(grayscale_image, (224, 224))  # Example size

    return resized_image

# Example usage:
image_file_path = 'path/to/image_document.png'
image_features = extract_features_from_image(image_file_path)
print(image_features.shape)  # Print shape of the extracted image features

# STEP3: MODEL TRAINING
import spacy
import re

# Load the spaCy model
nlp = spacy.load("en_core_web_sm")

# Extract dates and monetary values using regex
def extract_dates(text):
    dates = re.findall(r'\d{1,2}\/\d{1,2}\/\d{2,4}', text)
    return dates

def extract_money(text):
    money = re.findall(r'\$\d+(\.\d{1,2})?', text)
    return money

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd

# Load your dataset containing filenames and agreement versions
train_df = pd.read_csv("train.csv")

# Check the column names
print(train_df.columns)

# Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(train_df['File Name'], train_df['Aggrement Value'], test_size=0.2, random_state=42)

# Define pipeline
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),  # Convert filenames to TF-IDF features
    ('clf', RandomForestClassifier(n_estimators=100, random_state=42)),  # Random Forest classifier
])

# Fit pipeline
pipeline.fit(X_train, y_train)

# Evaluate model
y_pred = pipeline.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)
print(classification_report(y_test, y_pred))

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score

# Replace the following lines with your own data or data loading code
features = [[1], [2], [3], [4], [5], [6], [7], [8]]  # Your feature matrix
labels = [5000, 6000, 7000, 8000, 9000, 10000, 11000, 12000] # Your target vector

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)

# Initialize and train a Random Forest regressor
regressor = RandomForestRegressor(n_estimators=100, random_state=42)
regressor.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = regressor.predict(X_test)

# Calculate R^2 score (coefficient of determination)
r2 = r2_score(y_test, y_pred)
print("R^2 Score:", r2)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
import pandas as pd
from sklearn.model_selection import GridSearchCV

# Load your dataset, replace 'test.csv' with your actual dataset file
test = pd.read_csv('test.csv')

# Perform data preprocessing and feature engineering as needed
# For example, handle missing values, encode categorical variables, etc.

# Split the data into features (X) and target variable (y)
X = test.drop(columns=['Aggrement Value'])  # Features
y = test['Aggrement Value']  # Target variable

print(X.head())

# Encoding categorial variables
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
for col in X.select_dtypes(include='object'):
    X[col] = label_encoder.fit_transform(X[col])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Reinitialize the model and grid search objects
clf = RandomForestClassifier()
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20],
}
grid_search = GridSearchCV(clf, param_grid, cv=2, scoring='recall')
# Running the grid search
grid_search.fit(X_train, y_train)

# Print the best hyperparameters found by grid search
print(grid_search.best_params_)
print("Best Hyperparameters:", grid_search.best_params_)

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd

# Load text data from a CSV file
df = pd.read_csv("train.csv")

# Ensure that the column names are correct and match the DataFrame's column names
text_data = df['Party Two'].tolist()
labels = df['Aggrement Value'].tolist()

# Check the length of text_data and labels
print("Number of text samples:", len(text_data))
print("Number of labels:", len(labels))

# Ensure that each text sample has a corresponding label
assert len(text_data) == len(labels), "Number of text samples and labels must match"

# Splitting data
X_train, X_test, y_train, y_test = train_test_split(text_data, labels, test_size=0.2, random_state=42)

# Feature extraction using TF-IDF vectorizer
vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Initialize and train the classifier
classifier = LogisticRegression(max_iter=1000)  # Example classifier, you can choose another one
classifier.fit(X_train_tfidf, y_train)

# Predictions
y_pred = classifier.predict(X_test_tfidf)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Print classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

# STEP4: EVALUATING MODEL

# Assuming you have the ground truth metadata and predicted metadata as lists or dictionaries
# Example ground truth metadata
ground_truth_metadata = {
    'Agreement Value': [9000.0, 12000.0, 15000.0],
    'Agreement Start Date': ['01-04-2010', '01-04-2008', '07-07-2013'],
    # Add other fields...
}

#Eample predicted metadata
predicted_metadata = {
    'Agreement Value': [10000.0, 12000.0, 20000.],  # Example predictions
    'Agreement Start Date': ['01-04-2010', '01-04-2008', '07-07-2013'],  # Example predictions
    # Add other fields...
}

# Calculate per-field recall for each metadata field
per_field_recall = {}
for field in ground_truth_metadata.keys():
    true_positives = 0
    false_negatives = 0
    for true_value, pred_value in zip(ground_truth_metadata[field], predicted_metadata[field]):
        if true_value == pred_value:
            true_positives += 1
        else:
            false_negatives += 1
    recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0
    per_field_recall[field] = recall

# Print per-field recall scores
for field, recall in per_field_recall.items():
    print(f'Per-field Recall for {field}: {recall}')

# STEP5: RESULTFUL WEB SERVICE

from flask import Flask, request, jsonify
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd

# Load and preprocess the data
df = pd.read_csv("train.csv")
text_data = df['Party Two'].tolist()
labels = df['Aggrement Value'].tolist()

# Ensure that each text sample has a corresponding label
assert len(text_data) == len(labels), "Number of text samples and labels must match"

# Split the data
X_train, X_test, y_train, y_test = train_test_split(text_data, labels, test_size=0.2, random_state=42)

# Feature extraction using TF-IDF vectorizer
vectorizer = TfidfVectorizer(max_features=10000)  # Adjust max_features as needed
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

# Initialize and train the classifier
classifier = LogisticRegression(max_iter=1000)  # Example classifier, you can choose another one
classifier.fit(X_train_tfidf, y_train)

# Initialize Flask app
app = Flask(__name__)

# Define a route for the prediction endpoint
@app.route('/predict', methods=['POST'])
def predict():
    # Get the text data from the request
    data = request.json['data']

    # Perform feature extraction
    data_tfidf = vectorizer.transform(data)

    # Make predictions
    predictions = classifier.predict(data_tfidf)

    # Return the predictions as JSON
    return jsonify({'predictions': predictions.tolist()})

if __name__ == '__main__':
    app.run(debug=True)